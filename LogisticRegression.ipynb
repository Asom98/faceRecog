{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:27:19.131785Z",
     "start_time": "2024-10-03T19:23:02.147338Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(csv_file, base_folder, image_size=(64, 64)):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Remove \"Ahegao\" class if present\n",
    "    df = df[df['label'] != 'Ahegao']\n",
    "\n",
    "    # Initialize lists for images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through CSV, load images, and resize them\n",
    "    for index, row in df.iterrows():\n",
    "        img_path = os.path.join(base_folder, row['path'].strip())\n",
    "        label = row['label']\n",
    "\n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "            img = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            continue  # Skip corrupted or unreadable images\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Correct paths to your dataset\n",
    "csv_file = r\"./archive/data.csv\"\n",
    "base_folder = r\"./archive/dataset\"\n",
    "\n",
    "# Load images and labels\n",
    "X, y = load_data(csv_file, base_folder)\n",
    "\n",
    "\n",
    "### Feature Engineering ###\n",
    "# Step 1: Convert RGB images to grayscale\n",
    "def rgb_to_grayscale(images):\n",
    "    return np.dot(images[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "\n",
    "X_gray = rgb_to_grayscale(X)\n",
    "\n",
    "\n",
    "# Step 2: Extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        features = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "        hog_features.append(features)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "\n",
    "X_hog = extract_hog_features(X_gray)\n",
    "\n",
    "\n",
    "# Step 3: Apply PCA for dimensionality reduction\n",
    "def apply_pca(features, n_components=100):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(features)\n",
    "\n",
    "\n",
    "X_pca = apply_pca(X_hog, n_components=100)\n",
    "\n",
    "\n",
    "# Step 4: Normalize the features\n",
    "def normalize_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "X_normalized = normalize_features(X_pca)\n",
    "\n",
    "### Prepare labels ###\n",
    "# Encode labels to numerical values\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels for softmax regression\n",
    "y_one_hot = np.eye(len(np.unique(y_encoded)))[y_encoded]\n",
    "\n",
    "### Train/test split ###\n",
    "# Split into training, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6176187348889257\n",
      "Epoch 100, Loss: 1.0231170437853447\n",
      "Epoch 200, Loss: 0.9803738740234033\n",
      "Epoch 300, Loss: 0.9681932601909958\n",
      "Epoch 400, Loss: 0.9631262159740799\n",
      "Epoch 500, Loss: 0.9606134857012282\n",
      "Epoch 600, Loss: 0.9592263440365224\n",
      "Epoch 700, Loss: 0.958401162086725\n",
      "Epoch 800, Loss: 0.9578820089599873\n",
      "Epoch 900, Loss: 0.9575407372313467\n",
      "Epoch 1000, Loss: 0.9573083259432604\n",
      "Epoch 1100, Loss: 0.9571454081179529\n",
      "Epoch 1200, Loss: 0.9570284519182541\n",
      "Epoch 1300, Loss: 0.9569428216804059\n",
      "Epoch 1400, Loss: 0.9568790969937104\n",
      "Epoch 1500, Loss: 0.9568310294261427\n",
      "Epoch 1600, Loss: 0.9567943635231587\n",
      "Epoch 1700, Loss: 0.9567661330018743\n",
      "Epoch 1800, Loss: 0.9567442276336368\n",
      "Epoch 1900, Loss: 0.9567271192358284\n",
      "Validation Accuracy for fold: 0.6108771929824561\n",
      "Epoch 0, Loss: 1.6105512949757956\n",
      "Epoch 100, Loss: 1.0177918791016314\n",
      "Epoch 200, Loss: 0.9746052673976708\n",
      "Epoch 300, Loss: 0.9620468036370203\n",
      "Epoch 400, Loss: 0.9567295982846239\n",
      "Epoch 500, Loss: 0.9540543197610413\n",
      "Epoch 600, Loss: 0.952559735735481\n",
      "Epoch 700, Loss: 0.9516614638482683\n",
      "Epoch 800, Loss: 0.9510909559169415\n",
      "Epoch 900, Loss: 0.950712434217256\n",
      "Epoch 1000, Loss: 0.9504522095965786\n",
      "Epoch 1100, Loss: 0.9502680010730408\n",
      "Epoch 1200, Loss: 0.9501344108057624\n",
      "Epoch 1300, Loss: 0.9500355727113865\n",
      "Epoch 1400, Loss: 0.9499612290542361\n",
      "Epoch 1500, Loss: 0.949904543018566\n",
      "Epoch 1600, Loss: 0.9498608327752847\n",
      "Epoch 1700, Loss: 0.9498268143115077\n",
      "Epoch 1800, Loss: 0.9498001348430495\n",
      "Epoch 1900, Loss: 0.9497790772788616\n",
      "Validation Accuracy for fold: 0.5929824561403508\n",
      "Epoch 0, Loss: 1.6090178999783944\n",
      "Epoch 100, Loss: 1.0151451064173247\n",
      "Epoch 200, Loss: 0.9724449879428548\n",
      "Epoch 300, Loss: 0.9601574191749663\n",
      "Epoch 400, Loss: 0.9549784801138881\n",
      "Epoch 500, Loss: 0.9523723207932879\n",
      "Epoch 600, Loss: 0.9509116044946074\n",
      "Epoch 700, Loss: 0.9500292958773412\n",
      "Epoch 800, Loss: 0.9494657248480658\n",
      "Epoch 900, Loss: 0.9490896789305234\n",
      "Epoch 1000, Loss: 0.9488298148820397\n",
      "Epoch 1100, Loss: 0.9486450453925603\n",
      "Epoch 1200, Loss: 0.9485105661613733\n",
      "Epoch 1300, Loss: 0.9484107933465152\n",
      "Epoch 1400, Loss: 0.9483355920992427\n",
      "Epoch 1500, Loss: 0.9482781690221281\n",
      "Epoch 1600, Loss: 0.948233847920472\n",
      "Epoch 1700, Loss: 0.9481993340315373\n",
      "Epoch 1800, Loss: 0.9481722581092782\n",
      "Epoch 1900, Loss: 0.9481508859477071\n",
      "Validation Accuracy for fold: 0.5859649122807018\n",
      "Epoch 0, Loss: 1.6123772845183477\n",
      "Epoch 100, Loss: 1.0174514713458322\n",
      "Epoch 200, Loss: 0.974164916612797\n",
      "Epoch 300, Loss: 0.9615244559693317\n",
      "Epoch 400, Loss: 0.9561137472148193\n",
      "Epoch 500, Loss: 0.9533481276624063\n",
      "Epoch 600, Loss: 0.9517733008890648\n",
      "Epoch 700, Loss: 0.9508071057108975\n",
      "Epoch 800, Loss: 0.9501807658928406\n",
      "Epoch 900, Loss: 0.9497571621580994\n",
      "Epoch 1000, Loss: 0.9494609017174499\n",
      "Epoch 1100, Loss: 0.9492480228838094\n",
      "Epoch 1200, Loss: 0.9490916401639826\n",
      "Epoch 1300, Loss: 0.9489746460909528\n",
      "Epoch 1400, Loss: 0.9488857829929263\n",
      "Epoch 1500, Loss: 0.9488174259375942\n",
      "Epoch 1600, Loss: 0.9487642799302725\n",
      "Epoch 1700, Loss: 0.9487225871251566\n",
      "Epoch 1800, Loss: 0.9486896293660257\n",
      "Epoch 1900, Loss: 0.9486634074396713\n",
      "Validation Accuracy for fold: 0.5928395928395929\n",
      "Epoch 0, Loss: 1.6043231517915892\n",
      "Epoch 100, Loss: 1.018712381303074\n",
      "Epoch 200, Loss: 0.9760853701665878\n",
      "Epoch 300, Loss: 0.963724603567401\n",
      "Epoch 400, Loss: 0.9584918478182328\n",
      "Epoch 500, Loss: 0.955851279357016\n",
      "Epoch 600, Loss: 0.9543673977717556\n",
      "Epoch 700, Loss: 0.9534686037900889\n",
      "Epoch 800, Loss: 0.9528929301927481\n",
      "Epoch 900, Loss: 0.9525078924261989\n",
      "Epoch 1000, Loss: 0.9522413352046283\n",
      "Epoch 1100, Loss: 0.9520515929181897\n",
      "Epoch 1200, Loss: 0.9519134266418362\n",
      "Epoch 1300, Loss: 0.9518109212145389\n",
      "Epoch 1400, Loss: 0.9517336905323563\n",
      "Epoch 1500, Loss: 0.9516747529730756\n",
      "Epoch 1600, Loss: 0.951629293193051\n",
      "Epoch 1700, Loss: 0.9515939147614721\n",
      "Epoch 1800, Loss: 0.9515661748285869\n",
      "Epoch 1900, Loss: 0.9515442862325294\n",
      "Validation Accuracy for fold: 0.5967005967005967\n",
      "Average Cross-Validation Accuracy: 0.5958729501887398\n",
      "Epoch 0, Loss: 1.6119712488981475\n",
      "Epoch 100, Loss: 1.0227680019796441\n",
      "Epoch 200, Loss: 0.9802899902727151\n",
      "Epoch 300, Loss: 0.9681601170656842\n",
      "Epoch 400, Loss: 0.9631097935932224\n",
      "Epoch 500, Loss: 0.9606041730101054\n",
      "Epoch 600, Loss: 0.9592205033948942\n",
      "Epoch 700, Loss: 0.9583971790526501\n",
      "Epoch 800, Loss: 0.9578790862507188\n",
      "Epoch 900, Loss: 0.9575384493463589\n",
      "Final Test Accuracy: 0.6112280701754386\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression Implementation ###\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_size, num_classes, learning_rate=0.1,\n",
    "                 regularization=0.001):  # Best hyperparameters applied directly\n",
    "        self.W = np.random.randn(input_size, num_classes) * 0.01\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        cross_entropy_loss = -np.sum(y * np.log(y_hat + 1e-8)) / m\n",
    "        l2_loss = self.regularization * np.sum(np.square(self.W)) / 2  # L2 regularization term\n",
    "        return cross_entropy_loss + l2_loss\n",
    "\n",
    "    def compute_gradients(self, X, y, y_hat):\n",
    "        m = X.shape[0]\n",
    "        dW = np.dot(X.T, (y_hat - y)) / m + self.regularization * self.W  # L2 regularization gradient\n",
    "        db = np.sum(y_hat - y, axis=0, keepdims=True) / m\n",
    "        return dW, db\n",
    "\n",
    "    def train(self, X, y, epochs=1000):  # Best number of epochs applied directly\n",
    "        for i in range(epochs):\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_hat = self.softmax(z)\n",
    "\n",
    "            loss = self.compute_loss(y, y_hat)\n",
    "            dW, db = self.compute_gradients(X, y, y_hat)\n",
    "\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_hat = self.softmax(z)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "\n",
    "\n",
    "### Hyperparameter Selection Step - Commented Out ###\n",
    "# We are commenting out the random search section as we have already determined the best hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "# Random Search for Hyperparameter Tuning\n",
    "def random_search(param_dist, n_iter=10):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        params = {k: random.choice(v) for k, v in param_dist.items()}\n",
    "        accuracy = train_and_evaluate(params)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Random Search hyperparameter ranges\n",
    "param_dist = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'regularization': [0.1, 0.01, 0.001],\n",
    "    'epochs': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "# Perform random search with 10 iterations\n",
    "best_params_random, best_accuracy_random = random_search(param_dist, n_iter=10)\n",
    "print(f\"Best Hyperparameters from Random Search: {best_params_random}\")\n",
    "print(f\"Best Validation Accuracy from Random Search: {best_accuracy_random}\")\n",
    "\n",
    "# Save the best hyperparameters to reuse them later without hyperparameter tuning\n",
    "best_hyperparameters = best_params_random\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### K-Fold Cross-Validation ###\n",
    "def cross_validate_model(X, y, num_folds=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        model = LogisticRegression(input_size=X_train.shape[1], num_classes=y_train.shape[1], learning_rate=0.1,\n",
    "                                   regularization=0.001)\n",
    "        model.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "        # Validate the model\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = np.mean(np.argmax(y_val, axis=1) == y_val_pred)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Accuracy for fold: {val_accuracy}\")\n",
    "\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Average Cross-Validation Accuracy: {avg_accuracy}\")\n",
    "    return avg_accuracy\n",
    "\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cross_validation_accuracy = cross_validate_model(X_normalized, y_one_hot, num_folds=5)\n",
    "\n",
    "### Final Model Training with the Best Hyperparameters ###\n",
    "# Use the best hyperparameters directly\n",
    "final_model = LogisticRegression(input_size=X_train_full.shape[1], num_classes=y_train_full.shape[1], learning_rate=0.1,\n",
    "                                 regularization=0.001)\n",
    "final_model.train(X_train_full, y_train_full, epochs=1000)\n",
    "\n",
    "# Test the final model\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_accuracy = np.mean(np.argmax(y_test, axis=1) == y_test_pred)\n",
    "print(f\"Final Test Accuracy: {final_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
