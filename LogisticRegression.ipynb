{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:27:19.131785Z",
     "start_time": "2024-10-03T19:23:02.147338Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(csv_file, base_folder, image_size=(64, 64)):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Remove \"Ahegao\" class if present\n",
    "    df = df[df['label'] != 'Ahegao']\n",
    "\n",
    "    # Initialize lists for images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through CSV, load images, and resize them\n",
    "    for index, row in df.iterrows():\n",
    "        img_path = os.path.join(base_folder, row['path'].strip())\n",
    "        label = row['label']\n",
    "\n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "            img = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            continue  # Skip corrupted or unreadable images\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "# Correct paths to your dataset\n",
    "csv_file = r\"./archive/data.csv\"\n",
    "base_folder = r\"./archive/dataset\"\n",
    "\n",
    "# Load images and labels\n",
    "X, y = load_data(csv_file, base_folder)\n",
    "\n",
    "\n",
    "### Feature Engineering ###\n",
    "# Step 1: Convert RGB images to grayscale\n",
    "def rgb_to_grayscale(images):\n",
    "    return np.dot(images[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "\n",
    "X_gray = rgb_to_grayscale(X)\n",
    "\n",
    "\n",
    "# Step 2: Extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        features = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "        hog_features.append(features)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "\n",
    "X_hog = extract_hog_features(X_gray)\n",
    "\n",
    "\n",
    "# Step 3: Apply PCA for dimensionality reduction\n",
    "def apply_pca(features, n_components=100):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(features)\n",
    "\n",
    "\n",
    "X_pca = apply_pca(X_hog, n_components=100)\n",
    "\n",
    "\n",
    "# Step 4: Normalize the features\n",
    "def normalize_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "X_normalized = normalize_features(X_pca)\n",
    "\n",
    "### Prepare labels ###\n",
    "# Encode labels to numerical values\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels for softmax regression\n",
    "y_one_hot = np.eye(len(np.unique(y_encoded)))[y_encoded]\n",
    "\n",
    "### Train/test split ###\n",
    "# Split into training, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels for test data\n",
    "\n",
    "\n",
    "### Logistic Regression Implementation ###\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_size, num_classes, learning_rate=0.1,\n",
    "                 regularization=0.001):  # Best hyperparameters applied directly\n",
    "        self.W = np.random.randn(input_size, num_classes) * 0.01\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        cross_entropy_loss = -np.sum(y * np.log(y_hat + 1e-8)) / m\n",
    "        l2_loss = self.regularization * np.sum(np.square(self.W)) / 2  # L2 regularization term\n",
    "        return cross_entropy_loss + l2_loss\n",
    "\n",
    "    def compute_gradients(self, X, y, y_hat):\n",
    "        m = X.shape[0]\n",
    "        dW = np.dot(X.T, (y_hat - y)) / m + self.regularization * self.W  # L2 regularization gradient\n",
    "        db = np.sum(y_hat - y, axis=0, keepdims=True) / m\n",
    "        return dW, db\n",
    "\n",
    "    def train(self, X, y, epochs=1000):  # Best number of epochs applied directly\n",
    "        for i in range(epochs):\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_hat = self.softmax(z)\n",
    "\n",
    "            loss = self.compute_loss(y, y_hat)\n",
    "            dW, db = self.compute_gradients(X, y, y_hat)\n",
    "\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_hat = self.softmax(z)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "\n",
    "\n",
    "### Hyperparameter Selection Step - Commented Out ###\n",
    "# We are commenting out the random search section as we have already determined the best hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "# Random Search for Hyperparameter Tuning\n",
    "def random_search(param_dist, n_iter=10):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        params = {k: random.choice(v) for k, v in param_dist.items()}\n",
    "        accuracy = train_and_evaluate(params)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Random Search hyperparameter ranges\n",
    "param_dist = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'regularization': [0.1, 0.01, 0.001],\n",
    "    'epochs': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "# Perform random search with 10 iterations\n",
    "best_params_random, best_accuracy_random = random_search(param_dist, n_iter=10)\n",
    "print(f\"Best Hyperparameters from Random Search: {best_params_random}\")\n",
    "print(f\"Best Validation Accuracy from Random Search: {best_accuracy_random}\")\n",
    "\n",
    "# Save the best hyperparameters to reuse them later without hyperparameter tuning\n",
    "best_hyperparameters = best_params_random\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### K-Fold Cross-Validation ###\n",
    "def cross_validate_model(X, y, num_folds=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        model = LogisticRegression(input_size=X_train.shape[1], num_classes=y_train.shape[1], learning_rate=0.1,\n",
    "                                   regularization=0.001)\n",
    "        model.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "        # Validate the model\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = np.mean(np.argmax(y_val, axis=1) == y_val_pred)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Accuracy for fold: {val_accuracy}\")\n",
    "\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Average Cross-Validation Accuracy: {avg_accuracy}\")\n",
    "    return avg_accuracy\n",
    "\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cross_validation_accuracy = cross_validate_model(X_normalized, y_one_hot, num_folds=5)\n",
    "\n",
    "### Final Model Training with the Best Hyperparameters ###\n",
    "# Use the best hyperparameters directly\n",
    "final_model = LogisticRegression(input_size=X_train_full.shape[1], num_classes=y_train_full.shape[1], learning_rate=0.1,\n",
    "                                 regularization=0.001)\n",
    "final_model.train(X_train_full, y_train_full, epochs=1000)\n",
    "\n",
    "# Test the final model\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_accuracy = np.mean(np.argmax(y_test, axis=1) == y_test_pred)\n",
    "print(f\"Final Test Accuracy: {final_accuracy}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6152142065402426\n",
      "Epoch 100, Loss: 1.0237329530754582\n",
      "Epoch 200, Loss: 0.9809370798376079\n",
      "Epoch 300, Loss: 0.9686744394065568\n",
      "Epoch 400, Loss: 0.9635813781278232\n",
      "Epoch 500, Loss: 0.9610690161128623\n",
      "Epoch 600, Loss: 0.9596920462312584\n",
      "Epoch 700, Loss: 0.958879555781976\n",
      "Epoch 800, Loss: 0.9583725851514061\n",
      "Epoch 900, Loss: 0.9580418809822743\n",
      "Validation Accuracy for fold: 0.6126315789473684\n",
      "Epoch 0, Loss: 1.6128431898659845\n",
      "Epoch 100, Loss: 1.0193831768168873\n",
      "Epoch 200, Loss: 0.9760632027290654\n",
      "Epoch 300, Loss: 0.9634584778748244\n",
      "Epoch 400, Loss: 0.9581415401359136\n",
      "Epoch 500, Loss: 0.9554849713782537\n",
      "Epoch 600, Loss: 0.9540146791281156\n",
      "Epoch 700, Loss: 0.9531407241070363\n",
      "Epoch 800, Loss: 0.9525922983028655\n",
      "Epoch 900, Loss: 0.9522328901495598\n",
      "Validation Accuracy for fold: 0.5954385964912281\n",
      "Epoch 0, Loss: 1.6234253872516402\n",
      "Epoch 100, Loss: 1.0163655558212206\n",
      "Epoch 200, Loss: 0.9729699187450445\n",
      "Epoch 300, Loss: 0.9604506262676149\n",
      "Epoch 400, Loss: 0.9551882320807307\n",
      "Epoch 500, Loss: 0.9525572118069636\n",
      "Epoch 600, Loss: 0.951095709751372\n",
      "Epoch 700, Loss: 0.9502220181782988\n",
      "Epoch 800, Loss: 0.9496699404826547\n",
      "Epoch 900, Loss: 0.9493053900028006\n",
      "Validation Accuracy for fold: 0.5870175438596491\n",
      "Epoch 0, Loss: 1.6176030343796108\n",
      "Epoch 100, Loss: 1.0190786078735548\n",
      "Epoch 200, Loss: 0.9754541409610531\n",
      "Epoch 300, Loss: 0.9626816207266459\n",
      "Epoch 400, Loss: 0.9572303728548536\n",
      "Epoch 500, Loss: 0.9544618079033842\n",
      "Epoch 600, Loss: 0.952898952457608\n",
      "Epoch 700, Loss: 0.9519496494171753\n",
      "Epoch 800, Loss: 0.9513406379113398\n",
      "Epoch 900, Loss: 0.9509329031774233\n",
      "Validation Accuracy for fold: 0.5967005967005967\n",
      "Epoch 0, Loss: 1.6174786021427827\n",
      "Epoch 100, Loss: 1.0202814947516536\n",
      "Epoch 200, Loss: 0.9770937226306239\n",
      "Epoch 300, Loss: 0.9645469954925214\n",
      "Epoch 400, Loss: 0.9592356481963852\n",
      "Epoch 500, Loss: 0.9565600923048498\n",
      "Epoch 600, Loss: 0.9550611643127992\n",
      "Epoch 700, Loss: 0.9541567349290742\n",
      "Epoch 800, Loss: 0.9535797380214568\n",
      "Epoch 900, Loss: 0.9531951720403885\n",
      "Validation Accuracy for fold: 0.5998595998595999\n",
      "Average Cross-Validation Accuracy: 0.5983295831716884\n",
      "Epoch 0, Loss: 1.6151349207448324\n",
      "Epoch 100, Loss: 1.023580874912432\n",
      "Epoch 200, Loss: 0.9808729617492862\n",
      "Epoch 300, Loss: 0.9686416247142526\n",
      "Epoch 400, Loss: 0.9635627149456286\n",
      "Epoch 500, Loss: 0.9610576198788667\n",
      "Epoch 600, Loss: 0.9596846737087495\n",
      "Epoch 700, Loss: 0.958874541486346\n",
      "Epoch 800, Loss: 0.9583690212274001\n",
      "Epoch 900, Loss: 0.9580392476911603\n",
      "Final Test Accuracy: 0.6126315789473684\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
