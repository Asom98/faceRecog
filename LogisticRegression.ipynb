{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6105440751826163\n",
      "Epoch 100, Loss: 1.0228165208175526\n",
      "Epoch 200, Loss: 0.980322380079138\n",
      "Epoch 300, Loss: 0.9681703572995907\n",
      "Epoch 400, Loss: 0.9631022070100799\n",
      "Epoch 500, Loss: 0.9605844746694882\n",
      "Epoch 600, Loss: 0.9591927284396055\n",
      "Epoch 700, Loss: 0.9583637319736434\n",
      "Epoch 800, Loss: 0.9578413627332292\n",
      "Epoch 900, Loss: 0.957497301812286\n",
      "Validation Accuracy for fold: 0.6112280701754386\n",
      "Epoch 0, Loss: 1.613511604226043\n",
      "Epoch 100, Loss: 1.0182859052352555\n",
      "Epoch 200, Loss: 0.9749652276755921\n",
      "Epoch 300, Loss: 0.9623647473011693\n",
      "Epoch 400, Loss: 0.957025567325256\n",
      "Epoch 500, Loss: 0.9543380534983229\n",
      "Epoch 600, Loss: 0.9528367406568513\n",
      "Epoch 700, Loss: 0.9519348457247151\n",
      "Epoch 800, Loss: 0.9513624300794101\n",
      "Epoch 900, Loss: 0.9509829269611436\n",
      "Validation Accuracy for fold: 0.5992982456140351\n",
      "Epoch 0, Loss: 1.6101373401164965\n",
      "Epoch 100, Loss: 1.015201390729419\n",
      "Epoch 200, Loss: 0.9722600903119383\n",
      "Epoch 300, Loss: 0.9598866630287781\n",
      "Epoch 400, Loss: 0.9546685842129263\n",
      "Epoch 500, Loss: 0.9520417588133714\n",
      "Epoch 600, Loss: 0.950569021500034\n",
      "Epoch 700, Loss: 0.9496790881784671\n",
      "Epoch 800, Loss: 0.9491102651517205\n",
      "Epoch 900, Loss: 0.948730331183235\n",
      "Validation Accuracy for fold: 0.5936842105263158\n",
      "Epoch 0, Loss: 1.613677802355594\n",
      "Epoch 100, Loss: 1.0184496822937215\n",
      "Epoch 200, Loss: 0.974894582002143\n",
      "Epoch 300, Loss: 0.9621381062121649\n",
      "Epoch 400, Loss: 0.9566648106376867\n",
      "Epoch 500, Loss: 0.9538622792592147\n",
      "Epoch 600, Loss: 0.9522645103805989\n",
      "Epoch 700, Loss: 0.9512833371394205\n",
      "Epoch 800, Loss: 0.9506467066727092\n",
      "Epoch 900, Loss: 0.9502156722148063\n",
      "Validation Accuracy for fold: 0.5988065988065988\n",
      "Epoch 0, Loss: 1.6202613126881866\n",
      "Epoch 100, Loss: 1.0190833514998276\n",
      "Epoch 200, Loss: 0.9758564365484498\n",
      "Epoch 300, Loss: 0.9633278563323754\n",
      "Epoch 400, Loss: 0.9580191131353314\n",
      "Epoch 500, Loss: 0.9553368891293174\n",
      "Epoch 600, Loss: 0.9538277066284203\n",
      "Epoch 700, Loss: 0.9529123311662541\n",
      "Epoch 800, Loss: 0.9523250692489552\n",
      "Epoch 900, Loss: 0.9519314867491957\n",
      "Validation Accuracy for fold: 0.5984555984555985\n",
      "Average Cross-Validation Accuracy: 0.6002945447155973\n",
      "Epoch 0, Loss: 1.6139151730673966\n",
      "Epoch 100, Loss: 1.0225963689961413\n",
      "Epoch 200, Loss: 0.9802297989640427\n",
      "Epoch 300, Loss: 0.9681214758100611\n",
      "Epoch 400, Loss: 0.9630734073157653\n",
      "Epoch 500, Loss: 0.960566396551498\n",
      "Epoch 600, Loss: 0.959180842277731\n",
      "Epoch 700, Loss: 0.9583556220170609\n",
      "Epoch 800, Loss: 0.9578356560780188\n",
      "Epoch 900, Loss: 0.9574931793064\n",
      "Final Test Accuracy: 0.6108771929824561\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(csv_file, base_folder, image_size=(64, 64)):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Remove \"Ahegao\" class if present\n",
    "    df = df[df['label'] != 'Ahegao']\n",
    "    \n",
    "    # Initialize lists for images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through CSV, load images, and resize them\n",
    "    for index, row in df.iterrows():\n",
    "        img_path = os.path.join(base_folder, row['path'].strip())\n",
    "        label = row['label']\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB').resize(image_size)\n",
    "            img = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            continue  # Skip corrupted or unreadable images\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Correct paths to your dataset\n",
    "csv_file = r\"C:/Users/kasse/Documents/faceRecog/archive/data.csv\"\n",
    "base_folder = r\"C:/Users/kasse/Documents/faceRecog/archive/dataset\"\n",
    "\n",
    "# Load images and labels\n",
    "X, y = load_data(csv_file, base_folder)\n",
    "\n",
    "### Feature Engineering ###\n",
    "# Step 1: Convert RGB images to grayscale\n",
    "def rgb_to_grayscale(images):\n",
    "    return np.dot(images[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "X_gray = rgb_to_grayscale(X)\n",
    "\n",
    "# Step 2: Extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for img in images:\n",
    "        features = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "        hog_features.append(features)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "X_hog = extract_hog_features(X_gray)\n",
    "\n",
    "# Step 3: Apply PCA for dimensionality reduction\n",
    "def apply_pca(features, n_components=100):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(features)\n",
    "\n",
    "X_pca = apply_pca(X_hog, n_components=100)\n",
    "\n",
    "# Step 4: Normalize the features\n",
    "def normalize_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "X_normalized = normalize_features(X_pca)\n",
    "\n",
    "### Prepare labels ###\n",
    "# Encode labels to numerical values\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels for softmax regression\n",
    "y_one_hot = np.eye(len(np.unique(y_encoded)))[y_encoded]\n",
    "\n",
    "### Train/test split ###\n",
    "# Split into training, validation, and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels for test data\n",
    "\n",
    "### Logistic Regression Implementation ###\n",
    "class LogisticRegression:\n",
    "    def __init__(self, input_size, num_classes, learning_rate=0.1, regularization=0.001):  # Best hyperparameters applied directly\n",
    "        self.W = np.random.randn(input_size, num_classes) * 0.01\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, y, y_hat):\n",
    "        m = y.shape[0]\n",
    "        cross_entropy_loss = -np.sum(y * np.log(y_hat + 1e-8)) / m\n",
    "        l2_loss = self.regularization * np.sum(np.square(self.W)) / 2  # L2 regularization term\n",
    "        return cross_entropy_loss + l2_loss\n",
    "    \n",
    "    def compute_gradients(self, X, y, y_hat):\n",
    "        m = X.shape[0]\n",
    "        dW = np.dot(X.T, (y_hat - y)) / m + self.regularization * self.W  # L2 regularization gradient\n",
    "        db = np.sum(y_hat - y, axis=0, keepdims=True) / m\n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):  # Best number of epochs applied directly\n",
    "        for i in range(epochs):\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_hat = self.softmax(z)\n",
    "            \n",
    "            loss = self.compute_loss(y, y_hat)\n",
    "            dW, db = self.compute_gradients(X, y, y_hat)\n",
    "            \n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.b -= self.learning_rate * db\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {i}, Loss: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_hat = self.softmax(z)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "\n",
    "### Hyperparameter Selection Step - Commented Out ###\n",
    "# We are commenting out the random search section as we have already determined the best hyperparameters\n",
    "\n",
    "\"\"\"\n",
    "# Random Search for Hyperparameter Tuning\n",
    "def random_search(param_dist, n_iter=10):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        params = {k: random.choice(v) for k, v in param_dist.items()}\n",
    "        accuracy = train_and_evaluate(params)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Random Search hyperparameter ranges\n",
    "param_dist = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'regularization': [0.1, 0.01, 0.001],\n",
    "    'epochs': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "# Perform random search with 10 iterations\n",
    "best_params_random, best_accuracy_random = random_search(param_dist, n_iter=10)\n",
    "print(f\"Best Hyperparameters from Random Search: {best_params_random}\")\n",
    "print(f\"Best Validation Accuracy from Random Search: {best_accuracy_random}\")\n",
    "\n",
    "# Save the best hyperparameters to reuse them later without hyperparameter tuning\n",
    "best_hyperparameters = best_params_random\n",
    "\"\"\"\n",
    "\n",
    "### K-Fold Cross-Validation ###\n",
    "def cross_validate_model(X, y, num_folds=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # Train the model\n",
    "        model = LogisticRegression(input_size=X_train.shape[1], num_classes=y_train.shape[1], learning_rate=0.1, regularization=0.001)\n",
    "        model.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "        # Validate the model\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = np.mean(np.argmax(y_val, axis=1) == y_val_pred)\n",
    "        fold_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Accuracy for fold: {val_accuracy}\")\n",
    "\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Average Cross-Validation Accuracy: {avg_accuracy}\")\n",
    "    return avg_accuracy\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cross_validation_accuracy = cross_validate_model(X_normalized, y_one_hot, num_folds=5)\n",
    "\n",
    "### Final Model Training with the Best Hyperparameters ###\n",
    "# Use the best hyperparameters directly\n",
    "final_model = LogisticRegression(input_size=X_train_full.shape[1], num_classes=y_train_full.shape[1], learning_rate=0.1, regularization=0.001)\n",
    "final_model.train(X_train_full, y_train_full, epochs=1000)\n",
    "\n",
    "# Test the final model\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "final_accuracy = np.mean(np.argmax(y_test, axis=1) == y_test_pred)\n",
    "print(f\"Final Test Accuracy: {final_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
